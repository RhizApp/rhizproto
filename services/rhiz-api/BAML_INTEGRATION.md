# BAML Integration for Rhiz Protocol

**Type-safe, reliable AI-powered protocol features using BAML**

## Overview

Rhiz Protocol uses [BAML (Boundary ML)](https://www.boundaryml.com/) to provide AI-powered capabilities at the protocol layer. BAML ensures type safety, reliability, and excellent developer experience for LLM interactions.

### Why BAML?

- **Type Safety**: Generated TypeScript/Python types from `.baml` schemas
- **Reliability**: Automatic retries, fallbacks, and error handling
- **Testability**: Easy to mock and test AI functions
- **Observability**: Built-in tracing and monitoring
- **Multi-model**: Easy to switch between OpenAI, Anthropic, etc.
- **Better than LangChain**: Cleaner API, purpose-built for AI agents

## Protocol-Level AI Features

BAML powers three core protocol capabilities:

### 1. Relationship Extraction
Extract structured relationship data from unstructured text (bios, profiles, descriptions).

**Use Cases:**
- Importing LinkedIn connections
- Parsing relationship descriptions
- Extracting context from text

**BAML Functions:**
- `ExtractRelationshipsFromText` - Extract all relationships from text
- `AssessRelationshipQuality` - Validate relationship descriptions

### 2. Trust Explanations
Generate human-readable explanations of trust scores and conviction.

**Use Cases:**
- Explain why an entity has a certain trust score
- Describe conviction score for a relationship
- Explain path selection for introductions

**BAML Functions:**
- `ExplainTrustScore` - Break down trust metrics
- `ExplainConvictionScore` - Explain network confidence
- `ExplainPathChoice` - Justify path selection

### 3. Introduction Orchestration
AI-powered introduction facilitation through trust networks.

**Use Cases:**
- Generate personalized introduction requests
- Plan multi-step orchestration
- Assess introduction feasibility

**BAML Functions:**
- `GenerateIntroRequest` - Create intro request messages
- `GenerateForwardingIntro` - Create forwarding text
- `GenerateFollowup` - Create followup messages
- `PlanIntroductionOrchestration` - Plan multi-hop intros
- `AssessIntroductionFeasibility` - Evaluate intro likelihood

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│  Application Layer (FundRhiz, WeRhiz, etc.)             │
│  Uses protocol features, NOT re-implementing them        │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│  Protocol API (/api/v1/agents/*)                         │
│  REST endpoints for AI features                          │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│  Protocol Agent Service (app/services/protocol_agents.py)│
│  Python service wrapper with logging, error handling     │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│  BAML Generated Client (app/generated/baml_client/)      │
│  Type-safe Python functions generated from .baml files   │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│  BAML Definitions (baml_src/*.baml)                      │
│  - clients.baml: LLM provider config                     │
│  - relationship_extraction.baml: Extraction functions    │
│  - trust_explanations.baml: Explanation functions        │
│  - intro_orchestration.baml: Orchestration functions     │
└─────────────────────────────────────────────────────────┘
```

## Directory Structure

```
services/rhiz-api/
├── baml_src/                    # BAML source files
│   ├── clients.baml             # LLM client configuration
│   ├── generators.baml          # Code generation config
│   ├── relationship_extraction.baml
│   ├── trust_explanations.baml
│   └── intro_orchestration.baml
│
├── app/
│   ├── generated/               # Generated by BAML (don't edit)
│   │   └── baml_client/         # Python client code
│   │
│   ├── services/
│   │   └── protocol_agents.py   # Service wrapper
│   │
│   ├── api/
│   │   └── agents.py            # REST API endpoints
│   │
│   └── tests/
│       ├── test_protocol_agents.py  # Service tests
│       └── test_agent_api.py        # API tests
│
└── pyproject.toml               # baml-py dependency
```

## Development Workflow

### 1. Modify BAML Functions

Edit `.baml` files in `baml_src/`:

```baml
// baml_src/relationship_extraction.baml

class ExtractedRelationship {
  participant_a_name string
  participant_b_name string
  relationship_type string
  relationship_strength int  // 0-100
  context string
  confidence_score int
}

function ExtractRelationshipsFromText(
  text: string,
  context_hint: string?
) -> RelationshipExtractionResult {
  client GPT4
  prompt #"
    Extract all relationships from this text:
    {{ text }}
    
    Return structured JSON matching the schema.
  "#
}
```

### 2. Regenerate Python Code

```bash
cd services/rhiz-api
npx @boundaryml/baml generate --from ./baml_src
```

This generates type-safe Python code in `app/generated/baml_client/`.

### 3. Use in Service Layer

```python
# app/services/protocol_agents.py

from app.generated.baml_client import b
from app.generated.baml_client.types import RelationshipExtractionResult

class ProtocolAgentService:
    def __init__(self):
        self.client = b
    
    async def extract_relationships(self, text: str) -> RelationshipExtractionResult:
        result = await self.client.ExtractRelationshipsFromText(text=text)
        return result  # Type-safe!
```

### 4. Expose via API

```python
# app/api/agents.py

@router.post("/relationships/extract")
async def extract_relationships(request: ExtractRelationshipsRequest):
    agent_service = get_protocol_agent_service()
    result = await agent_service.extract_relationships_from_text(request.text)
    return result
```

### 5. Test

```python
# app/tests/test_protocol_agents.py

@pytest.mark.asyncio
async def test_extract_relationships(agent_service, mock_baml_client):
    mock_baml_client.ExtractRelationshipsFromText = AsyncMock(return_value=mock_result)
    result = await agent_service.extract_relationships_from_text("test")
    assert result.total_found == 1
```

## API Usage Examples

### Extract Relationships from Text

```bash
POST /api/v1/agents/relationships/extract
Content-Type: application/json

{
  "text": "Alice and Bob co-founded TechCo in 2020 and worked together for 3 years",
  "context_hint": "Professional relationships"
}
```

Response:
```json
{
  "relationships": [
    {
      "participant_a_name": "Alice",
      "participant_b_name": "Bob",
      "relationship_type": "professional",
      "relationship_strength": 85,
      "context": "Co-founded TechCo, worked together 3 years",
      "confidence_score": 90
    }
  ],
  "total_found": 1,
  "extraction_quality": 85,
  "ambiguous_cases": []
}
```

### Explain Trust Score

```bash
POST /api/v1/agents/trust/explain
Content-Type: application/json

{
  "entity_did": "did:plc:alice123",
  "trust_metrics": {
    "trustScore": 88,
    "reputation": 91,
    "reciprocity": 85,
    "consistency": 89
  },
  "network_context": {
    "averageTrustScore": 75
  }
}
```

Response:
```json
{
  "overall_trust_score": 88,
  "explanation_summary": "High trust based on strong network reputation and consistent behavior",
  "strengths": [
    "Reputation score (91) is well above network average",
    "High reciprocity indicates mutual relationships",
    "Consistent behavior over time"
  ],
  "concerns": [],
  "comparison_to_network": "17% above average",
  "trend": "stable",
  "recommendation": "Highly trustworthy entity"
}
```

### Generate Introduction Request

```bash
POST /api/v1/agents/intros/generate-request
Content-Type: application/json

{
  "requester_did": "did:plc:alice",
  "requester_context": "Founder of TechCo, building AI infrastructure",
  "intermediary_did": "did:plc:carol",
  "intermediary_context": "Former colleague at BigCorp",
  "target_did": "did:plc:bob",
  "target_context": "Investor at Sequoia Capital",
  "introduction_purpose": "Seeking Series A investment for TechCo",
  "relationship_data": {
    "alice_carol_strength": 85,
    "carol_bob_strength": 78
  }
}
```

Response:
```json
{
  "recipient_name": "Carol",
  "subject_line": "Introduction request: Alice → Bob",
  "message_body": "Hi Carol,\n\nHope you're doing well! I wanted to reach out about a potential introduction...",
  "success_probability": 75,
  "personalization_score": 85,
  "optimal_send_time": "2025-10-24T10:00:00Z",
  "followup_timing_days": 5
}
```

## Configuration

### Environment Variables

```bash
# Required
OPENAI_API_KEY=sk-...

# Optional (defaults shown)
OPENAI_MODEL=gpt-4-turbo-preview
```

### Client Configuration

Edit `baml_src/clients.baml` to add more providers or adjust settings:

```baml
client<llm> GPT4 {
  provider openai
  options {
    model gpt-4-turbo-preview
    api_key env.OPENAI_API_KEY
    temperature 0.7
    max_tokens 2000
  }
}

client<llm> Claude {
  provider anthropic
  options {
    model claude-3-sonnet-20240229
    api_key env.ANTHROPIC_API_KEY
    temperature 0.7
  }
}

// Use fallback: tries GPT4 first, falls back to Claude
function MyFunction(input: string) -> Output {
  client GPT4
  client Claude  // Fallback
  prompt #"..."#
}
```

## Testing

### Run All Tests

```bash
cd services/rhiz-api
poetry run pytest app/tests/test_protocol_agents.py -v
poetry run pytest app/tests/test_agent_api.py -v
```

### Test with Coverage

```bash
poetry run pytest --cov=app.services.protocol_agents --cov=app.api.agents --cov-report=html
```

### Mock BAML Calls

```python
from unittest.mock import AsyncMock, patch

@patch("app.services.protocol_agents.b")
async def test_my_function(mock_baml_client):
    mock_baml_client.MyFunction = AsyncMock(return_value=mock_result)
    # Test your code
```

## Deployment

### 1. Install Dependencies

```bash
poetry install
```

### 2. Generate BAML Client

```bash
npx @boundaryml/baml generate --from ./baml_src
```

### 3. Set Environment Variables

```bash
export OPENAI_API_KEY=sk-...
export OPENAI_MODEL=gpt-4-turbo-preview
```

### 4. Run Service

```bash
poetry run uvicorn app.main:app --host 0.0.0.0 --port 8000
```

## Monitoring & Observability

BAML provides built-in tracing. Enable with environment variables:

```bash
BAML_LOG_LEVEL=debug
BAML_TRACE=true
```

Logs will show:
- Function calls with inputs/outputs
- Token usage per call
- Latency metrics
- Retry attempts
- Error traces

## Best Practices

### 1. Keep BAML Functions Protocol-Level

✅ **DO**: Extract relationships, explain trust, orchestrate intros
❌ **DON'T**: Analyze pitches, match investors (app-specific)

### 2. Use Type-Safe Schemas

```baml
class MyOutput {
  score int  // Use specific types
  status string  // Not "any" or generic
}
```

### 3. Test with Mocks

Don't call real LLMs in tests - use mocks:

```python
mock_baml_client.MyFunction = AsyncMock(return_value=expected_output)
```

### 4. Handle Errors Gracefully

```python
try:
    result = await self.client.MyFunction(input=data)
except Exception as e:
    logger.error(f"BAML function failed: {e}")
    # Provide fallback or raise
```

### 5. Version BAML Files

Commit `.baml` files to git, but add to `.gitignore`:
```
app/generated/baml_client/
```

Regenerate on deployment.

## Troubleshooting

### Version Mismatch Error

```
Error: Generator version (0.72.0) !== baml package version (0.211.2)
```

**Fix**: Update version in `baml_src/generators.baml`:
```baml
generator target {
  version "0.211.2"  // Match installed baml-py version
}
```

### Import Errors

```
ModuleNotFoundError: No module named 'app.generated'
```

**Fix**: Regenerate BAML client:
```bash
npx @boundaryml/baml generate --from ./baml_src
```

### OpenAI API Errors

```
Error: OpenAI API key not found
```

**Fix**: Set environment variable:
```bash
export OPENAI_API_KEY=sk-...
```

## Resources

- **BAML Documentation**: https://docs.boundaryml.com/
- **BAML GitHub**: https://github.com/BoundaryML/baml
- **OpenAI API**: https://platform.openai.com/docs/api-reference

## Contributing

When adding new AI features:

1. Define BAML function in appropriate `.baml` file
2. Regenerate Python client
3. Add method to `ProtocolAgentService`
4. Create API endpoint in `app/api/agents.py`
5. Write tests (service + API)
6. Update this documentation

## License

BAML integration follows the same dual-license as Rhiz Protocol:
- MIT License
- Apache License 2.0

